diff --git a/model/sam_hybrid_adapter.py b/model/sam_hybrid_adapter.py
index 0000000..1111111 100644
--- a/model/sam_hybrid_adapter.py
+++ b/model/sam_hybrid_adapter.py
@@ -1,6 +1,7 @@
 import math
 import torch
 import torch.nn as nn
+from collections import OrderedDict
 import torch.nn.functional as F
 from torch.nn.parameter import Parameter
 from model.segment_anything.modeling import Sam
@@ -235,30 +236,155 @@ class LoRA_Sam(nn.Module):
             nn.init.kaiming_uniform_(self.fa_ti_v_proj_A.weight, a=math.sqrt(5))
             nn.init.zeros_(self.fa_ti_v_proj_B.weight)
 
     def save_lora_parameters(self, filename: str):
-        # Implementation to save both LoRA weights and Full FT weights (if any)
-        # For simplicity, we save the entire state_dict if FT is enabled for a component,
-        # or just LoRA/Prompt/Decoder weights if not.
-        # But consistent with existing code, let's extract relevant parts.
-
-        state_dict = self.sam.state_dict()
-        save_dict = {}
-
-        # Save LoRA Layers
-        for i, m in enumerate(self.w_As):
-            save_dict[f"w_a_{i:03d}"] = m.weight
-        for i, m in enumerate(self.w_Bs):
-            save_dict[f"w_b_{i:03d}"] = m.weight
-        # ... and other LoRA storage ...
-
-        # If we are in FT mode, we probably want to save the whole component weight
-        # But trainers usually handle this via full model saving.
-        # This function seems specifically for LoRA parameters.
-        # I'll stay consistent with existing save_lora_parameters in other files.
-        torch.save(state_dict, filename)  # Saving whole thing for hybrid safety
+        assert str(filename).endswith(".pt") or str(filename).endswith(".pth")
+
+        # 1) LoRA weights (same key convention as sam_lora_image_encoder_mask_decoder.py)
+        lora_tensors = {}
+        lora_tensors.update(
+            {f"w_a_{i:03d}": self.w_As[i].weight for i in range(len(self.w_As))}
+        )
+        lora_tensors.update(
+            {f"w_b_{i:03d}": self.w_Bs[i].weight for i in range(len(self.w_Bs))}
+        )
+        lora_tensors.update(
+            {f"sa_a_{i:03d}": self.self_attn_As[i].weight for i in range(len(self.self_attn_As))}
+        )
+        lora_tensors.update(
+            {f"sa_b_{i:03d}": self.self_attn_Bs[i].weight for i in range(len(self.self_attn_Bs))}
+        )
+        lora_tensors.update(
+            {f"cti_a_{i:03d}": self.cross_attn_ti_As[i].weight for i in range(len(self.cross_attn_ti_As))}
+        )
+        lora_tensors.update(
+            {f"cti_b_{i:03d}": self.cross_attn_ti_Bs[i].weight for i in range(len(self.cross_attn_ti_Bs))}
+        )
+        lora_tensors.update(
+            {f"cit_a_{i:03d}": self.cross_attn_it_As[i].weight for i in range(len(self.cross_attn_it_As))}
+        )
+        lora_tensors.update(
+            {f"cit_b_{i:03d}": self.cross_attn_it_Bs[i].weight for i in range(len(self.cross_attn_it_Bs))}
+        )
+        if getattr(self, "decoder_lora", False):
+            lora_tensors.update(
+                {
+                    "fati_qa": self.fa_ti_q_proj_A.weight,
+                    "fati_qb": self.fa_ti_q_proj_B.weight,
+                    "fati_va": self.fa_ti_v_proj_A.weight,
+                    "fati_vb": self.fa_ti_v_proj_B.weight,
+                }
+            )
+
+        # 2) prompt_encoder + mask_decoder(head only; transformer 제외)도 같이 저장 (레퍼런스와 동일)
+        sam_state = self.sam.state_dict()
+        prompt_encoder_tensors = {}
+        mask_decoder_tensors = {}
+        for k, v in sam_state.items():
+            if "prompt_encoder" in k:
+                prompt_encoder_tensors[k] = v
+            if "mask_decoder" in k and "transformer" not in k:
+                mask_decoder_tensors[k] = v
+
+        merged = {**lora_tensors, **prompt_encoder_tensors, **mask_decoder_tensors}
+
+        # 3) FT 모드면 전체 sam_state_dict도 함께 저장(재현/재개용)
+        #    LoRA-only loader들도 위의 top-level 키들은 그대로 읽을 수 있음(추가 키 무시).
+        if getattr(self, "encoder_ft", False) or getattr(self, "decoder_ft", False):
+            merged["__sam_state_dict__"] = sam_state
+            merged["__format__"] = "hybrid_lora_v1"
+            merged["__mode__"] = getattr(self, "mode", None)
+
+        torch.save(merged, filename)
 
     def load_lora_parameters(self, filename: str):
-        self.sam.load_state_dict(torch.load(filename))
+        assert str(filename).endswith(".pt") or str(filename).endswith(".pth")
+
+        ckpt = torch.load(filename, map_location="cpu")
+
+        # Case A) 우리가 저장한 hybrid dict: __sam_state_dict__가 있으면 우선 전체를 로드
+        if isinstance(ckpt, dict) and "__sam_state_dict__" in ckpt:
+            self.sam.load_state_dict(ckpt["__sam_state_dict__"], strict=False)
+            return
+
+        # Case B) full SAM state_dict로 저장된 파일(이전 코드): 키가 LoRA-prefix가 아니면 그대로 로드
+        if isinstance(ckpt, dict) and not any(
+            k.startswith(("w_a_", "w_b_", "sa_a_", "sa_b_", "cti_a_", "cti_b_", "cit_a_", "cit_b_", "fati_"))
+            for k in ckpt.keys()
+        ):
+            self.sam.load_state_dict(ckpt, strict=False)
+            return
+
+        # Case C) LoRA-only 포맷(레퍼런스와 동일): ModuleList들에 weight를 주입 + prompt/mask head 갱신
+        def _copy_linear_weights(linears: nn.ModuleList, prefix: str):
+            for i, layer in enumerate(linears):
+                key = f"{prefix}{i:03d}"
+                if key not in ckpt:
+                    continue
+                with torch.no_grad():
+                    layer.weight.copy_(ckpt[key].to(layer.weight.device))
+
+        _copy_linear_weights(self.w_As, "w_a_")
+        _copy_linear_weights(self.w_Bs, "w_b_")
+        _copy_linear_weights(self.self_attn_As, "sa_a_")
+        _copy_linear_weights(self.self_attn_Bs, "sa_b_")
+        _copy_linear_weights(self.cross_attn_ti_As, "cti_a_")
+        _copy_linear_weights(self.cross_attn_ti_Bs, "cti_b_")
+        _copy_linear_weights(self.cross_attn_it_As, "cit_a_")
+        _copy_linear_weights(self.cross_attn_it_Bs, "cit_b_")
+
+        if getattr(self, "decoder_lora", False):
+            for k_attr, k_ckpt in [
+                ("fa_ti_q_proj_A", "fati_qa"),
+                ("fa_ti_q_proj_B", "fati_qb"),
+                ("fa_ti_v_proj_A", "fati_va"),
+                ("fa_ti_v_proj_B", "fati_vb"),
+            ]:
+                if k_ckpt not in ckpt or not hasattr(self, k_attr):
+                    continue
+                layer = getattr(self, k_attr)
+                with torch.no_grad():
+                    layer.weight.copy_(ckpt[k_ckpt].to(layer.weight.device))
+
+        # prompt_encoder + mask_decoder(head only) 로드 (device mismatch 방지 위해 현재 param device로 이동)
+        sam_state = self.sam.state_dict()
+        updated = {}
+        for k, ref_v in sam_state.items():
+            if "prompt_encoder" in k and k in ckpt:
+                updated[k] = ckpt[k].to(ref_v.device)
+            if "mask_decoder" in k and "transformer" not in k and k in ckpt:
+                updated[k] = ckpt[k].to(ref_v.device)
+        if updated:
+            sam_state.update(updated)
+            self.sam.load_state_dict(sam_state, strict=False)
 
     def forward(self, batched_input, multimask_output, image_size):
         return self.sam(batched_input, multimask_output, image_size)